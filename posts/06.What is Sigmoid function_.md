# 06.What is Sigmoid function?



# 什么是Sigmoid？

<br />一提起Sigmoid function可能大家的第一反应就是Logistic Regression。我们把一个sample扔进 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320197-0ec9442a-db8f-44bf-9f80-e0b9ebfce1e3.svg#align=left&display=inline&height=20&originHeight=20&originWidth=66&size=0&status=done&style=none&width=66) 中，就可以输出一个probability，也就是是这个sample属于第一类或第二类的概率。<br />
<br />还有像神经网络也有用到 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320171-15a5a8dd-cbd0-419d-b3dd-afbaac77b350.svg#align=left&display=inline&height=20&originHeight=20&originWidth=66&size=0&status=done&style=none&width=66) ，不过在那里叫activation function。<br />
<br />Sigmoid function长下面这个样子：<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320171-9755010b-3522-468c-ba2b-38e638887762.svg#align=left&display=inline&height=52&originHeight=52&originWidth=600&size=0&status=done&style=none&width=600)<br />
<br />其实这个function我们只知道怎么用它，但是不知道它是怎么来的，以及底层的含义是什么。知乎有人解答不过都是照着教材抄一抄捞几个赞，那么我详细的解释一下，争取不要让算法工程师沦为调参工程师。<br />
<br />首先假设我们有两个class： ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320159-4c20421b-e329-4023-990c-2c54fb9ff442.svg#align=left&display=inline&height=20&originHeight=20&originWidth=22&size=0&status=done&style=none&width=22) 和 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320410-8ff7ebc9-daa8-4b40-b373-4f78ebea62ca.svg#align=left&display=inline&height=20&originHeight=20&originWidth=22&size=0&status=done&style=none&width=22) ，并且给出一个sample ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320138-bd738038-1919-4e83-bce0-549b26ab572c.svg#align=left&display=inline&height=13&originHeight=13&originWidth=11&size=0&status=done&style=none&width=11) ，我们的目标是求 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320170-28fbc607-e7ca-40e7-9dca-511d58e390f7.svg#align=left&display=inline&height=13&originHeight=13&originWidth=11&size=0&status=done&style=none&width=11) 属于 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320197-228583c8-e757-4f07-a873-948c4b3965fb.svg#align=left&display=inline&height=20&originHeight=20&originWidth=22&size=0&status=done&style=none&width=22) 的概率是多少。<br />
<br />这个概率我们可以通过Naive Bayes很轻松的得出，也就是（公式1）：<br />
<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320584-4bcc7aa9-e283-46ab-bab5-91e31c727769.svg#align=left&display=inline&height=60&originHeight=60&originWidth=600&size=0&status=done&style=none&width=600)<br />
<br />其中等号右面的分布这项（公式2）：<br />
<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320183-a67f2d31-11c5-43fa-a40b-aba52105e1b2.svg#align=left&display=inline&height=40&originHeight=40&originWidth=600&size=0&status=done&style=none&width=600)<br />这个公式是高中难度的，不过也解释一下： ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320445-76883126-4be2-411d-b78a-dff766c77569.svg#align=left&display=inline&height=13&originHeight=13&originWidth=11&size=0&status=done&style=none&width=11) 出现的概率等于， ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320202-083930d4-bd1f-4d95-9320-f12a99050885.svg#align=left&display=inline&height=20&originHeight=20&originWidth=22&size=0&status=done&style=none&width=22) 出现的概率乘以 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320450-6010b1d2-2895-4eb7-a3bf-ac21eeebaf9f.svg#align=left&display=inline&height=20&originHeight=20&originWidth=22&size=0&status=done&style=none&width=22) 中出现 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320198-89e95545-ab8c-4221-b101-ea923423ecc4.svg#align=left&display=inline&height=13&originHeight=13&originWidth=11&size=0&status=done&style=none&width=11) 的概率 加上 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320219-26cd2040-5dc9-4328-bbb4-4b2aafb1751b.svg#align=left&display=inline&height=20&originHeight=20&originWidth=22&size=0&status=done&style=none&width=22)出现的概率乘以 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320221-b23e3413-61f9-4594-8715-82c72708748b.svg#align=left&display=inline&height=20&originHeight=20&originWidth=22&size=0&status=done&style=none&width=22) 中出现 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320198-ea2d33c6-2039-49f4-8a41-f249f5739cfa.svg#align=left&display=inline&height=13&originHeight=13&originWidth=11&size=0&status=done&style=none&width=11) 的概率。<br />
<br />那么就可以把公式2带入公式1的分母中（公式3）：<br />
<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320258-28965f72-99e8-4c58-8c34-a419d86c5213.svg#align=left&display=inline&height=60&originHeight=60&originWidth=600&size=0&status=done&style=none&width=600)<br />
<br />下面我们将等式两边同时除以分子就变成了（公式4）：<br />
<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320211-1d7af836-706f-4e72-bbe6-5139860a6c56.svg#align=left&display=inline&height=84&originHeight=84&originWidth=600&size=0&status=done&style=none&width=600)<br />
<br />设 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320216-da83ad51-7b63-4d91-ba61-ef2a6222abe5.svg#align=left&display=inline&height=52&originHeight=52&originWidth=175&size=0&status=done&style=none&width=175)<br />
<br />那么把 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320533-9adc665a-e2fc-40fc-8429-abbbc8273f09.svg#align=left&display=inline&height=13&originHeight=13&originWidth=9&size=0&status=done&style=none&width=9) 带入公式4就变成了：<br />
<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320264-ad42e348-f299-452a-a096-96afc5ed6ba4.svg#align=left&display=inline&height=52&originHeight=52&originWidth=600&size=0&status=done&style=none&width=600)<br />也就是Sigmoid function<br />
<br />

# 更多思考
上面已经知道 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320406-2cb89e57-4f2f-47e2-95de-e7329d707cb3.svg#align=left&display=inline&height=20&originHeight=20&originWidth=66&size=0&status=done&style=none&width=66) 函数是从什么东西推导过来的了，那么有个问题就是，既然上面式子中只有 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320244-75f32762-6701-411d-9839-4e1425688bd1.svg#align=left&display=inline&height=23&originHeight=23&originWidth=66&size=0&status=done&style=none&width=66) 和 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320291-70a682ec-ba8a-47b8-ba0d-94e3cbb5d35a.svg#align=left&display=inline&height=23&originHeight=23&originWidth=66&size=0&status=done&style=none&width=66) 我们不知道，那我们干脆用Bayes不就能直接计算出 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320265-0385a98d-403b-4d59-90d5-12eda5bcbe75.svg#align=left&display=inline&height=23&originHeight=23&originWidth=66&size=0&status=done&style=none&width=66) 了嘛？<br />（x是某个sample，其中有多个feature，也就是说x是一个vector）<br />
<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320334-4d214111-9101-41b7-800c-c2ec4ceb45b4.svg#align=left&display=inline&height=40&originHeight=40&originWidth=600&size=0&status=done&style=none&width=600)<br />
<br />但是Bayes有一个限制条件就是所有的feature都必须是independent的，假如我们训练的sample中各个feature都是independent的话，那么Bayes会给我们一个很好的结果。但实际情况下这是不太可能的，各个feature之间不可能是independent的，那么bias就会非常大，搞出的model就很烂。<br />
<br />
<br />

# 这个 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320291-3522c7aa-ffcf-4f88-a20b-3f617e09db0d.svg#align=left&display=inline&height=13&originHeight=13&originWidth=9&size=0&status=done&style=none&width=9) 应该长什么样子
我们将 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320560-1c9768ca-340b-4a28-b93e-03cd7c33ddd3.svg#align=left&display=inline&height=13&originHeight=13&originWidth=9&size=0&status=done&style=none&width=9) 变换一下可以变换成下面的样子：<br />
<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320248-23df6e9e-cead-4737-bd04-22dee22b6dc9.svg#align=left&display=inline&height=60&originHeight=60&originWidth=600&size=0&status=done&style=none&width=600)<br />
<br />上式中 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320256-4941eba3-d154-4295-a9d1-e7641cba24de.svg#align=left&display=inline&height=52&originHeight=52&originWidth=75&size=0&status=done&style=none&width=75) 中的 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320270-c859e33c-9c87-48bf-8ff3-7bdfc497675b.svg#align=left&display=inline&height=52&originHeight=52&originWidth=57&size=0&status=done&style=none&width=57) 是很好求的，设 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320444-771ed3cb-bd31-4fcf-9fea-5ce6507b152f.svg#align=left&display=inline&height=20&originHeight=20&originWidth=22&size=0&status=done&style=none&width=22) 在训练集中出现的数目是 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320331-4a6cb0c0-0eb5-4753-915c-06d3c4afccdd.svg#align=left&display=inline&height=20&originHeight=20&originWidth=23&size=0&status=done&style=none&width=23) ， ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320337-f58045c7-c9ad-4cbe-b9d6-140c289f9e2c.svg#align=left&display=inline&height=20&originHeight=20&originWidth=22&size=0&status=done&style=none&width=22) 在训练集中出现的数目是 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320334-50f845a8-3b38-4422-932a-2870dd21d5fe.svg#align=left&display=inline&height=20&originHeight=20&originWidth=23&size=0&status=done&style=none&width=23) ，那么：<br />
<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320647-13e44d41-3b9d-4b33-9a1c-47d5eafc4620.svg#align=left&display=inline&height=105&originHeight=105&originWidth=600&size=0&status=done&style=none&width=600)<br />
<br />其中 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320278-17bb870b-0221-48a6-b46c-9df9df69445b.svg#align=left&display=inline&height=23&originHeight=23&originWidth=66&size=0&status=done&style=none&width=66) 和 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320286-bb7100f3-7b6f-4fd0-8c72-ac3ceb05f883.svg#align=left&display=inline&height=23&originHeight=23&originWidth=66&size=0&status=done&style=none&width=66)都遵从Guassian probability distribution：<br />
<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320355-0212606b-1d0b-4299-9640-72c3f8ec0405.svg#align=left&display=inline&height=60&originHeight=60&originWidth=600&size=0&status=done&style=none&width=600)<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320388-2b6dc8bf-44e2-4b73-9f11-cf46c0ec2d27.svg#align=left&display=inline&height=60&originHeight=60&originWidth=600&size=0&status=done&style=none&width=600)<br />
<br />那么我们再回到这个公式中：<br />
<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320328-45b3926c-b38c-4f88-8509-5cb5ff9a61c5.svg#align=left&display=inline&height=60&originHeight=60&originWidth=600&size=0&status=done&style=none&width=600)<br />
<br />第二项我们已经求出来了，下面我们把第一项Guassian probability distribution带入：<br />
<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320392-9faf92cf-99a6-4302-9411-1eb8e069705b.svg#align=left&display=inline&height=119&originHeight=119&originWidth=600&size=0&status=done&style=none&width=600)<br />
<br />乍一看，我滴妈简直太复杂太恶心了 :)<br />但是别慌，很多东西都能消掉的，我们来消一下。<br />首先，上面分子分母中 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320326-6face15e-7987-4593-b10a-a090e2665a77.svg#align=left&display=inline&height=52&originHeight=52&originWidth=73&size=0&status=done&style=none&width=73) 可以消掉，就变成了：<br />
<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249321021-bcff337c-88a7-440a-b0a5-67c13b00a9bc.svg#align=left&display=inline&height=119&originHeight=119&originWidth=600&size=0&status=done&style=none&width=600)<br />
<br />接着拆：<br />
<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320375-c78c8f0c-ffb8-4aa8-b588-ee660ee10da7.svg#align=left&display=inline&height=68&originHeight=68&originWidth=600&size=0&status=done&style=none&width=600)<br />
<br />再拆：<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320458-7f415258-c2bd-4223-a518-37b1ba4bdf01.svg#align=left&display=inline&height=68&originHeight=68&originWidth=669&size=0&status=done&style=none&width=669)<br />
<br />上式中第二项 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320342-e980f500-626d-4667-a8bb-4cd0b1dd11a8.svg#align=left&display=inline&height=41&originHeight=41&originWidth=451&size=0&status=done&style=none&width=451) ，中括号里面有两项，我再把这两项里面的括号全都打开，打开的目的是为了后面的化简，首先先看第一项：<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320437-90dc71ce-1e71-4b34-9de9-4c5c419af63b.svg#align=left&display=inline&height=43&originHeight=43&originWidth=795&size=0&status=done&style=none&width=795)<br />
<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320585-1a1788ac-5baa-4cb4-91f4-ddfe8034329f.svg#align=left&display=inline&height=43&originHeight=43&originWidth=600&size=0&status=done&style=none&width=600)<br />
<br />第二项化简方法一样，把下角标换成2就行了：<br />
<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320465-c72065ba-9bd9-4e8c-8122-7208e1099aae.svg#align=left&display=inline&height=43&originHeight=43&originWidth=600&size=0&status=done&style=none&width=600)<br />
<br />拆的差不多了，下面我们回到 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320365-4d38337d-a7e2-4c51-8c60-bb5b0554f599.svg#align=left&display=inline&height=52&originHeight=52&originWidth=223&size=0&status=done&style=none&width=223) 中，把刚才的化简结果带进去：<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320687-5396e238-7206-43b5-b74a-e2c2a1a2cea7.svg#align=left&display=inline&height=60&originHeight=60&originWidth=772&size=0&status=done&style=none&width=772)<br />
<br />仔细观察不难发现，上式中中括号里面第一项和第四项是可以消掉的。<br />并且我们可以认为 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320384-467ea7db-95c9-484c-b2f0-60c919b535dd.svg#align=left&display=inline&height=20&originHeight=20&originWidth=107&size=0&status=done&style=none&width=107) ，刚才我一直没解释 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320425-6fe34f0e-1a09-4a0e-958f-05764114584a.svg#align=left&display=inline&height=17&originHeight=17&originWidth=11&size=0&status=done&style=none&width=11) 和 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320424-8c960b19-e05c-49c1-9e7f-c42cd0f8b6fe.svg#align=left&display=inline&height=17&originHeight=17&originWidth=13&size=0&status=done&style=none&width=13) 是什么，下面我简单说一下， ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320456-90e138fb-e64d-4959-8b7b-4773b442e0e7.svg#align=left&display=inline&height=17&originHeight=17&originWidth=11&size=0&status=done&style=none&width=11) 就是mean（均值）， ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320427-f6f14901-003d-48c7-934e-e4f7a9485823.svg#align=left&display=inline&height=17&originHeight=17&originWidth=13&size=0&status=done&style=none&width=13) 就是covairance（协方差），其中![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320433-43e695f3-7473-440f-818e-e62a4003b301.svg#align=left&display=inline&height=17&originHeight=17&originWidth=11&size=0&status=done&style=none&width=11)是个vector，![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320408-4fdfb0aa-890e-479d-803e-eab4ae5aaf8e.svg#align=left&display=inline&height=17&originHeight=17&originWidth=13&size=0&status=done&style=none&width=13)是个matrix，具体什么形式不在本文里详细解释，一解释就没完没了了，可以深推一下Guassian看看paper（个人感觉意义不大，其实理解到这里完全够用了）。<br />
<br />好了，为什么可以认为 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320688-e84753da-3ed6-409e-9afb-099c80eeada3.svg#align=left&display=inline&height=20&originHeight=20&originWidth=107&size=0&status=done&style=none&width=107) 呢？因为如果每个class都有自己的covariance的话，那么variance会很大，参数的量一下就上去了，参数一多，就容易overfitting。这么说的话， ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320461-0555bcc8-cc62-44a0-9f76-656ab3810ece.svg#align=left&display=inline&height=13&originHeight=13&originWidth=9&size=0&status=done&style=none&width=9) 里面的第一项 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320524-35a98204-0ddf-42f1-b403-95eb80a6f32a.svg#align=left&display=inline&height=60&originHeight=60&originWidth=79&size=0&status=done&style=none&width=79) 就是0了。<br />
<br />好开心，又有好多东西被约掉了 :)<br />
<br />最后， ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320488-fc277921-ba1e-4335-8d95-5682843718a4.svg#align=left&display=inline&height=13&originHeight=13&originWidth=9&size=0&status=done&style=none&width=9) 被化简成了下面这种最终形态：<br />
<br />![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320484-fab521f5-e44d-4fdf-aaa5-0cbeab4db30a.svg#align=left&display=inline&height=52&originHeight=52&originWidth=600&size=0&status=done&style=none&width=600)<br />
<br />可以观察到，第一项有系数 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320539-24b438c9-a59d-4cce-9e86-01c77c24414c.svg#align=left&display=inline&height=13&originHeight=13&originWidth=11&size=0&status=done&style=none&width=11) ，后面几项里其实都是参数。<br />我们就可以理解为x的系数其实就是 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320539-736a02fa-77d8-4685-a4ad-a636f788f280.svg#align=left&display=inline&height=20&originHeight=20&originWidth=66&size=0&status=done&style=none&width=66) 中的参数 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320510-56f3a017-3aa2-47a2-a9fd-a60e07b4422e.svg#align=left&display=inline&height=21&originHeight=21&originWidth=24&size=0&status=done&style=none&width=24) （这是个matrix），后面那些项可以看成是参数 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320758-a1376535-6b60-4f62-a37d-2272214c22cb.svg#align=left&display=inline&height=17&originHeight=17&originWidth=8&size=0&status=done&style=none&width=8) 。<br />
<br />那么在Generative model中我们的目标是寻找最佳的 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320527-989b4b25-bfdd-48fb-a964-a612d83cc30b.svg#align=left&display=inline&height=21&originHeight=21&originWidth=133&size=0&status=done&style=none&width=133) 使 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320529-b28fb330-4c0e-4616-b979-08b48d91a422.svg#align=left&display=inline&height=23&originHeight=23&originWidth=66&size=0&status=done&style=none&width=66) maximise。<br />
<br />但是我们已经将一连串复杂的参数和方程化简成了 ![](https://cdn.nlark.com/yuque/0/2020/svg/1173836/1586249320582-30e5dc37-41c2-4607-9204-92e261e721e9.svg#align=left&display=inline&height=25&originHeight=25&originWidth=124&size=0&status=done&style=none&width=124) 那为什么还要舍近求远的求5个参数去将目标最优化呢？只有“两个参数”的方法我们叫做Discraminative model。<br />
<br />实际上，在大多数情况下，这两种方法各有利弊，但是实际上Discraminative model泛化能力比Generative model还是强不少的。什么时候Generative model更好呢？<br />
<br />- _training data比较少的时候，需要靠几率模型脑补没有发生或的事情。_<br />- _training data中有noise。_<br />
<br />

