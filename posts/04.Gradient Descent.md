# 04.Gradient Descent

# Review

<br />在ML的step3中，我们要找一个最好的function，这是一个optimization problem（最优化问题）<br />
<br />在step2中我们定义了 Loss $$L$$ ，它是function的function，将一组参数 $$\theta$$ 带入Loss function $$\theta^*=arg\min_{\theta} L(\theta)$$ 中，那么它就会告诉你这组参数它有多不好。

解这件事情我们可以用GD去做，假设这组 $$\theta$$ 有两个参数$$\{\theta_{1}, \theta_{2}\}$$：

1. 随机取一组起始点：$$\theta^0=\begin{bmatrix} \theta_{1}^0 \\  \theta_{2}^0\end{bmatrix}$$，上标0表示这是初始化的第1组参数，下标1和2表示这是这组参数的第一个参数和第二个参数。

1. 计算 $$\theta_{1}^0$$ 和 $$\theta_{2}^0$$ 对 $$L$$ 的偏微分，然后用$$\begin{bmatrix} \theta_{1}^0 \\  \theta_{2}^0\end{bmatrix}$$减去learning rate $$\eta$$ 乘以这个偏微分，得到一组新的参数：<br />$$\begin{bmatrix} \theta_{1}^1 \\  \theta_{2}^1\end{bmatrix}=\begin{bmatrix} \theta_{1}^0 \\  \theta_{2}^0\end{bmatrix}-\eta\begin{bmatrix} \partial L(\theta_{1}^0)/\partial\theta_{1} \\  \partial L(\theta_{2}^0)/\partial\theta_{2} \end{bmatrix}$$，同理$$\begin{bmatrix} \theta_{1}^1 \\  \theta_{2}^1\end{bmatrix}$$中的上标1，表示在第2个时间点，由 $$\theta^0$$ 更新之后的参数。其中$$\begin{bmatrix} \partial L(\theta_{1}^0)/\partial\theta_{1} \\  \partial L(\theta_{2}^0)/\partial\theta_{2} \end{bmatrix}$$可以简写成$$\triangledown L(\theta)$$，这个倒三角型叫做Gridient是一个vector，因此上式又可以简写成：$$\theta^1=\theta^0-\eta \triangledown L(\theta^0)$$

1. 下一组参数更新：$$\begin{bmatrix} \theta_{1}^2 \\  \theta_{2}^2\end{bmatrix}=\begin{bmatrix} \theta_{1}^1 \\  \theta_{2}^1\end{bmatrix}-\eta\begin{bmatrix} \partial L(\theta_{1}^1)/\partial\theta_{1} \\  \partial L(\theta_{2}^1)/\partial\theta_{2} \end{bmatrix}$$，简化后为：$$\theta^2=\theta^1-\eta \triangledown L(\theta^1)$$

1. 反复进行上面的操作......

下面我们讲一下GD的一些小技巧：
# Tip1: Tuning your learning rates
小心的调整你的lr，有是有lr是会给你带来一些问题的。

假如我们的lr调的刚刚好的话，那Loss会顺着下图的红色箭头一步一步走到最低点。<br />![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585550623911-e25c6f41-6c17-48d1-8220-b1c5f109e28b.png#align=left&display=inline&height=270&name=image.png&originHeight=1270&originWidth=1130&size=218211&status=done&style=none&width=240)<br />但如果你的lr调的太小的话，那么走的速度就会非常的慢，虽然如果有足够多的时间走到local optimal的地方，但你可能没法接受这件事：<br />![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585550737705-fb034e65-5cd4-44a7-acd7-e9aba4df393f.png#align=left&display=inline&height=277&name=image.png&originHeight=1274&originWidth=1104&size=249012&status=done&style=none&width=240)<br />如果你的lr调的稍微大了一点，像绿色箭头的话，就好像变成了个巨人，步伐太大了，它永远都没办法走到那个特别低的地方，会在山谷口这里震荡：<br />![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585550842287-e77f70bc-a00f-46d1-9a5f-a3f70cb0851f.png#align=left&display=inline&height=277&name=image.png&originHeight=1254&originWidth=1086&size=310775&status=done&style=none&width=240)

## Adaptive Learning Rates


调lr还是蛮困难的，那么有没有什么自动的方法帮助我们调整lr呢？<br />
<br />一个基本大原则：lr要随着每一轮epoch的迭代而变得越来越小。

- 刚开始在起始点的时候，距离最低点的距离是比较远的，所以步伐可以走的大一点。
- 但是经过几次参数的update之后，你已经比较靠近这个最低点了，所以这个时候应该减小你的lr，让它收敛在最低点的地方。



### Adagrad
最好的方案是对每个不同的参数选用不同的lr，其中最容易实做，最简单的是Adagrad。<br />
<br />定义：每一个参数的lr，都把它除上之前算出来的微分值的root mean square（均方根）<br />
<br />之前我们更新参数是按照这种方式：$$w^{t+1} \leftarrow w^t-\eta^tg^t$$ （注意这里的$$w$$表示一个参数而不是一组，因为在adagrad中每一个参数都有不同的lr，所以这里面我们只用一个参数举例）

在Adagrad中我们是这么做的：$$w^{t+1} \leftarrow w^t-\frac{\eta^t}{\sigma^t}g^t$$<br />$$g^t$$是偏微分的值：$$g^t=\frac{\partial L(\theta^t)}{\partial w}$$<br />$$\eta^t$$会随着update次数而更新：$$\eta^t=\eta / \sqrt{t+1}$$<br />$$\sigma^t$$是过去所有的微分值的root mean square，这个值对所有的参数都是不一样的，所以不同的参数的lr都是不一样的。

**下面我们来一个例子，看看到底是怎么实做的：**

- $$w^1 \leftarrow w^0-\frac{\eta^0}{\sigma^0}g^0$$，假设初始参数是$$w^0$$，这一点的微分是$$g^0$$，它的lr是$$\frac{\eta^0}{\sigma^0}$$，其中$$\sigma^0=\sqrt{(g^0)^2}$$
- $$w^2 \leftarrow w^1-\frac{\eta^1}{\sigma^1}g^1$$，$$\sigma^1=\sqrt{\frac{1}{2}[(g^0)^2+(g^1)^2]}$$
- $$w^3 \leftarrow w^2-\frac{\eta^2}{\sigma^2}g^2$$，$$\sigma^2=\sqrt{\frac{1}{3}[(g^0)^2+(g^1)^2+(g^2)^2]}$$
- ......
- $$w^{t+1} \leftarrow w^t-\frac{\eta^t}{\sigma^t}g^t$$，$$\sigma^t=\sqrt{\frac{1}{t+1} \sum_{i=0}^{t} (g^i)^2}$$
- 我们发现$$\eta^t=\eta / \sqrt{t+1}$$ 和 $$\sigma^t=\sqrt{\frac{1}{t+1} \sum_{i=0}^{t} (g^i)^2}$$ 分母都有一个$$\sqrt{t+1}$$，所以是可以消掉的

消掉之后我们得到：$$w^{t+1} \leftarrow w^t-\frac{\eta}{\sqrt{\sum_{i=0}^t (g^i)^2}}g^t$$ 



其实Adagrad只是比较简单而已，实际上目前最好的lr调整方案还是adam，但是相对麻烦一些，就不展开了。

# Tip2: Stochastic Gradient Descent
**它可以让你的训练速度更快！**

我们先来回顾一下Regression中的Loss：

$$L=\sum_{n}(\hat y^n-(b+\sum w_{i}x_{i}^n))^2$$

这个Loss很合理，因为它考虑了所有的样本，然后就去做GD就可以了：$$\theta^i=\theta^{i-1}-\eta\triangledown L(\theta^{i-1})$$

但Stochastic Gradient Descent（随机梯度下降）的想法不太一样，它每次只拿一个$$x^n$$出来，可以随机取（如果你的error surface比较崎岖，随机取会有一些帮助）也可以按顺序取，**此时的Loss只考虑一个样本**：

$$L^n=(\hat y^n-(b+\sum w_{i}x_{i}^n))^2$$<br />
<br />接下来在更新参数的时候只考虑那一个样本：$$\theta^i=\theta^{i-1}-\eta\triangledown L^n(\theta^{i-1})$$，一句话总结就是**随机梯度下降看一个样本更新一次参数**。

**SGD到底哪里比较好？**

GD是看完所有的样本更新一次参数，就像下图这样，它其实是比较稳定的，走的方向是按照Gradient建议我们的方向去走的：<br />![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585569706168-5cdee60d-757d-46a2-80dc-9d3c9298492c.png#align=left&display=inline&height=247&name=image.png&originHeight=906&originWidth=1176&size=1172768&status=done&style=none&width=320)<br />而SGD是看一个样本更新一次参数，所以同样是看了$$n$$个样本，SGD就已经更新了$$n$$次参数，就像下图这样：<br />![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585569799882-71248e67-0ba3-4f18-8354-9dd743b3a6ed.png#align=left&display=inline&height=237&name=image.png&originHeight=868&originWidth=1068&size=1209275&status=done&style=none&width=292)

# Tip3: Feature Scaling


假如现在我有两个特征，这两个特征的数值分布的很不一致，那么就需要将他们的分布调整成尽量一致的状态：<br />![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585570113644-22f0f151-8b26-47b4-982a-21ef4509fb89.png#align=left&display=inline&height=226&name=image.png&originHeight=942&originWidth=2024&size=637107&status=done&style=none&width=485)

**为什么要这么做呢？**

假如还是这个线性回归的场景：$$y=b+w_1x_1+w_2x_2$$<br />![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585570977848-7019eff4-586f-49d5-b825-89cc825a92d6.png#align=left&display=inline&height=175&name=image.png&originHeight=538&originWidth=1218&size=257646&status=done&style=none&width=397)

这个时候你会发现，$$w_1$$对$$y$$的变化是比较小的，$$w_2$$对$$y$$的变化是比较大的，这个error surface可能是这个样子：<br />![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585571111550-1339f815-4d99-4c14-ab97-9f84a8ea2310.png#align=left&display=inline&height=211&name=image.png&originHeight=710&originWidth=1304&size=557248&status=done&style=none&width=387)

如果你把$$x_1,x_2$$收缩到相同的区间内，那么error surface就会比较接近圆形：<br />![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585571231918-ba951799-5614-420c-ac8b-8ac471b5634c.png#align=left&display=inline&height=215&name=image.png&originHeight=654&originWidth=998&size=404346&status=done&style=none&width=328)

 什么圆形的surface会比较好呢？我们可以看到，椭圆形的surface在GD的时候会拐一下，那么一般的lr就很难搞定，因为横向和纵向需要两个不一样的lr，也就是不同的参数需要不一样的lr来适应这种比较奇怪的error surface<br />![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585571322351-3f3f0339-67f4-4392-9081-36d7438ba7f3.png#align=left&display=inline&height=227&name=image.png&originHeight=710&originWidth=1240&size=595300&status=done&style=none&width=396)

## 如何做Feature Scaling?
方法有千百种，选一个自己自己习喜欢的就好。

常见的做法是这样的：<br />假设一共有$$R
$$个样本，每个样本都是一个vector，对每个维度（feature）计算一个均值$$m_i$$和标准差$$\sigma_i$$，如下图![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585580528743-b08b1d6c-d18f-4ae5-ba14-55c250c39cd7.png#align=left&display=inline&height=410&name=image.png&originHeight=820&originWidth=2198&size=1256018&status=done&style=none&width=1099)

然后比如对第$$r$$个样本的第$$i$$个特征做下面这个公式：<br />$$x_i^r \leftarrow \frac{x_i^r-m_i}{\sigma_i}$$

当你对所有的样本、特征都做完了之后，那么所有特征的均值都是0，方差都是1。

# Gradient Descent的理论基础


我们不管Gradient Descent，我们先来想想看，如果在下图的这个等高线图中找一个最低点，你应该会怎么做：<br />![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585581192076-99022649-0860-4d2c-9fc8-cccc27e14ecb.png#align=left&display=inline&height=326&name=image.png&originHeight=1202&originWidth=1528&size=1948334&status=done&style=none&width=415)

如果给我一个起始点$$\theta^0$$，我们有办法在这个起始点周围画一个圆圈，然后在这个红色圈圈里面找一个最低点：<br />![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585581330494-472a0c89-b928-4708-bb74-6ff8b6ccb678.png#align=left&display=inline&height=331&name=image.png&originHeight=1192&originWidth=1424&size=2029100&status=done&style=none&width=395)

同理我们可以再走下一步$$\theta^1$$，再一步$$\theta^2$$，一直这么走下去：<br />![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585581449418-7483fbd1-00b3-4fcb-aa67-c06586340f94.png#align=left&display=inline&height=330&name=image.png&originHeight=1196&originWidth=1428&size=2148267&status=done&style=none&width=394)<br />
<br />现在的问题是：**如何在这个红色圈圈里面找一个让Loss最小的参数呢？这就要从Taylor Series（泰勒级数）说起**<br />**<br />**
## Taylor Series


泰勒级数告诉我们：任何一个function $$h(x)$$ 在$$x=x_0$$位置是infinitely differentiable（无穷次可微/光滑函数），那么你可以将$$h(x)$$写成下面的样子：

$$h(x)=\sum_{k=0}^{\infty} \frac{h^{(k)}(x_0)}{k!}(x-x_0)^k$$<br />$$=h(x_0)+h'(x_0)(x-x_0)+\frac{h''(x_0)}{2!}(x-x_0)^2+...$$<br />
<br />$$h^{(k)}(x_0)$$表示在$$h(x_0)$$微分$$k$$以后的值

当$$x$$很接近$$x_0$$的时候，$$(x-x_0)$$会远大于$$(x-x_0)^2 ...$$会大于后面的3次4次无穷多次...所以这时候可以把后面的高次项都删掉：

$$h(x)\approx h(x_0)+h'(x_0)(x-x_0)$$

**举个例子：**<br />假设$$h(x)=\sin(x)$$，当$$x_0= \pi/4$$，那么可以写成下图这样：<br />![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585653334278-36492181-a0e4-4e3f-9097-26c1878b57cd.png#align=left&display=inline&height=248&name=image.png&originHeight=496&originWidth=2330&size=635598&status=done&style=none&width=1165)

我们将它画出来的话会得到下图这个结果（橙色线是sin）：<br />![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585653378337-44f5e1ee-6c2d-44ca-ac49-f0e1c5adeb59.png#align=left&display=inline&height=295&name=image.png&originHeight=1070&originWidth=1654&size=801848&status=done&style=none&width=456)<br />我们发现，如果$$x$$很接近$$\pi/4$$的话，那么后面的2次方项、3次方项......都很小，所以就可以忽略它们只考虑一次的部分。

## Multivariable Taylor Series


如果有多个参数，并且$$x$$很接近$$x_0$$，$$y$$很接近$$y_0$$的计划，那么泰勒级数展开可以写成下面的样子，只保留一次项，多次项都可以忽略不计了：

$$h(x,y) \approx h(x_0,y_0)+\frac{\partial h(x_0,y_0)}{\partial x}(x-x_0)+\frac{\partial h(x_0,y_0)}{\partial y}(y-y_0)$$

## 再回到Gradient Descent


如果给我们一个中心点$$(a,b)$$，我画了一个很小很小的红色圆圈：<br />![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585654169643-c5da7724-270a-460c-81a9-f96eb087a9e4.png#align=left&display=inline&height=258&name=image.png&originHeight=886&originWidth=1142&size=1344417&status=done&style=none&width=332)

在这个红色圆圈之内，我其实可以把Loss function用泰勒级数做简化：

$$L(\theta)\approx L(a,b)+\frac{\partial L(a,b)}{\partial \theta_1}(\theta_1-a)+\frac{\partial L(a,b)}{\partial \theta_2}(\theta_2-b)$$

这三项都是常数，所以我们用一个符号表示它们：

$$s=L(a,b)\\u=\frac{\partial L(a,b)}{\partial \theta_1}\\v=\frac{\partial L(a,b)}{\partial \theta_2}$$

那么式子就可以简化成这样：<br /> <br />$$L(\theta)\approx s+u(\theta_1-a)+v(\theta_2-b)$$

回过头来，我们下面要在红色圈圈里找让Loss最小的$$\theta_1,\theta_2$$，假设红色圈圈的半径是$$d$$：

$$(\theta_1-a)^2+(\theta_2-a)^2 \le d^2$$

再设$$\vartriangle\theta_1=\theta_1-a$$，$$\vartriangle\theta_2=\theta_2-a$$：<br />![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585655259622-89408f3a-6f6a-4268-ac74-0e8c9436148f.png#align=left&display=inline&height=90&name=image.png&originHeight=268&originWidth=954&size=106748&status=done&style=none&width=320)<br />![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585655290141-78b01e6a-9e1f-4c29-bb85-9ca8599b01b0.png#align=left&display=inline&height=189&name=image.png&originHeight=704&originWidth=1026&size=239346&status=done&style=none&width=275)

就变成了，如果我们想让Loss最小呢，如何选择$$\vartriangle\theta_1$$和$$\vartriangle\theta_2$$呢？非常简单，就像下图这样选择$$(u,v)$$相反方向，再把它增长到红色圈圈的边缘就可以了，如下图：<br />![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585655419943-5f0724fb-ffbf-41a0-b378-8841ef98da3c.png#align=left&display=inline&height=185&name=image.png&originHeight=634&originWidth=944&size=236021&status=done&style=none&width=275)

公式就是$$\begin{bmatrix} \vartriangle\theta_1\\\vartriangle\theta_2 \end{bmatrix}=-\eta \begin{bmatrix} u\\v\end{bmatrix}$$，这个$$\eta$$就是增长蓝色箭头到红色圈圈边缘的倍数，我们把$$\vartriangle\theta_1$$和$$\vartriangle\theta_2$$还原一下就是：$$\begin{bmatrix} \theta_1\\\theta_2 \end{bmatrix}= \begin{bmatrix} a\\b\end{bmatrix} -\eta \begin{bmatrix} u\\v\end{bmatrix}$$，就是中心点$$(a,b)$$减去某一个常数$$\eta$$乘以$$(u,v)$$

接下来要做的事就是将$$u=\frac{\partial L(a,b)}{\partial \theta_1},v=\frac{\partial L(a,b)}{\partial \theta_2}$$代进去：

$$\begin{bmatrix} \theta_1\\\theta_2 \end{bmatrix}= \begin{bmatrix} a\\b\end{bmatrix} -\eta \begin{bmatrix} u\\v\end{bmatrix}=\begin{bmatrix} a\\b\end{bmatrix} -\eta \begin{bmatrix} \frac{\partial L(a,b)}{\partial \theta_1} \\ \frac{\partial L(a,b)}{\partial \theta_2}\end{bmatrix}$$<br />
<br />这不就是Gradient Descent吗！！！$$\eta$$不就是learning rate吗！！！<br />
<br />但是上式要成立是有个前提的，就是你画的那个红色圈圈要足够小时，泰勒级数估计才足够的精确，所以说这个lr就不能太大，理论来说，lr需要无穷小才行，但是我们实做的时候没必要这样，就足够小就可以了。也就是说，如果你的lr没有设好，比如设大了，就会导致Loss不收敛。<br />
<br />上面我们讨论的是泰勒级数展开成一次式，那有没有可能展开成二次式呢？是有的，比如牛顿法，但是一般实做中尤其是在DL实做中，这个方法并不是太普及，因为要算二次微分，会增加非常多的运算，这些运算在DL中是无法承受的，非常不划算。<br />

