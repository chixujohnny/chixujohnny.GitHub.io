# 02.Regression

# Regression: Output a scalar

<br />Machine Learning要做的事情就是找一个$$function
$$，那么Regression要做的事情就是希望这function output的是一个数值scalar，下面有几个例子：

- 股票大盘预测
- 无人驾驶
- 推荐问题



## Linear model

<br />形如：$$y=b+\sum w_{i}x_{i}$$   <br />$$x_{i}$$表示input的各种样本的attribute（身高、体重等等），我们叫做feature特征。<br />$$w_{i}$$：weight<br />$$b_{i}$$：bias

## Loss function


Loss function $$L$$ 是用来衡量function是不是够好的，可以理解为是function的function<br />input：function，output：这个function有多不好<br />可以写成：$$L(f)=L(w,b)$$  因为function $$f$$ 是由参数 $$b$$ 跟 $$w$$ 决定的，因此 $$L$$ 也就是在衡量一组参数的好坏，也就是衡量 $$b$$ 跟 $$w$$ 的好坏。

那么如何定义一个Loss function呢？实际上可以根据你自己的喜好自己来定义一个合理的function，那我们可以用一个比较经典的方式定义 $$L$$：<br /><br />
$$=\sum_{n-1}^k (\widehat{y}^n-(b+w\cdot x_{cp}^n))^2$$<br />
<br />其中$$x_{cp}$$表示这个样本的特征（不用介意cp是什么意思，就是随便起的一个名字），$$n$$表示第n个样本，$$k$$ 表示样本数量，$$\widehat{y}$$ 是真实的数值，$$b+w\cdot x_{cp}^n$$ 表示预测的数值。<br /><br />上式就表示，对每一个样本，用真实的数值减去预测的数值后取平方再求和，这个叫做**估测误差。**<br /><br />因此估测误差越大就表示这个function越不好（这组参数 $$b$$ 跟 $$w$$ 越不好）<br />

## Pick the "Best" Function

<br />上面我们已经能衡量function的好坏了，下面我们就要从这个function set中挑选出最好的function：<br />
<br />$$f^*=arg \min_{f} L(f)$$<br />
<br />上式的含义是，寻找一个可以让 $$L(f)$$ 最小的 $$f$$ 并写作 $$f^*$$<br />或者也可以像下面这种写法：<br />
<br />$$w^*,b^*=arg \min_{w,b} L(w,b)$$<br />
<br />穷举所有的 $$b$$ 跟 $$w$$，看哪一组 $$b$$ 跟 $$w$$ 可以让 $$Loss$$ 最小，最好的这一组写作$$w^*,b^*$$<br />带入Loss function之后可以写成：<br /><br />
$$w^*,b^*=arg \min_{w,b} L(w,b)$$<br /><br />$$=arg \min_{w,b} \sum (\widehat{y}^n-(b+w\cdot x_{cp}^n))^2$$

## Gradient Descent

<br />那么上式的最好的 $$b$$ 跟 $$w$$ 应该怎么求呢？这里要使用一种方法叫做Gradient Descent梯度下降法，它厉害的地方在于，**只要** $$L$$ **是可微分的**，不管它是什么function，Gradient Descent都可以拿来处理。下面我们来看看GD是怎么做的：

先假设我们有一个非常简单的task，$$L(w)$$只有一个参数 $$w$$，这个$$L(w)$$ 可以是任何一种function，只要是可微分的就行。那么现在的问题就是 $$w^*=arg \min_{w} L(w)$$ 找一个 $$w$$ 让这个$$L(w)$$最小。

**当只有一个参数时GD是怎么做的：**

- 随机选取一个初始的点 $$w^0$$（其实有一些比随机更好的方案，不过我们之后再讨论）
- $$\frac{\mathrm{d}y}{\mathrm{d}x} \mid w=w^0$$ 计算 $$w=w^0$$ 这个位置，参数 $$w$$ 对 $$L$$ 的微分（微分可以理解为切线斜率）
- 假如切线斜率是负数，表示Loss左边高右边低，那么你需要增加 $$w$$ 的值，反之则需要减少 $$w$$ 的值。
- 下面就要往右（左）踏一步，这一步踏多少取决于这个公式 $$\eta \frac{\mathrm{d}y}{\mathrm{d}x} \mid w=w^0$$ ，取决于第一件事情：$$\frac{\mathrm{d}y}{\mathrm{d}x} \mid w=w^0$$越大表示目前处于越陡峭的位置，移动的距离就越大。第二件事情：取决于常数项 $$\eta$$ 叫做learning rate，可以理解为参数更新的幅度，如果lr比较大那么学的就比较快，但是不精细，反之较慢但是比较精细。所以说，此时我们已经知道在 $$w^0$$ 这个位置需要让它更新$$\eta \frac{\mathrm{d}y}{\mathrm{d}x} \mid w=w^0$$这么多的量。
- 把$$w^0$$更新至$$w^1$$: $$w^1 \leftarrow w^0-\eta \frac{\mathrm{d}y}{\mathrm{d}x} \mid w=w^0$$
- 重复上面的操作，再次计算 $$w^1$$ 位置的微分值并乘以learning rate：$$\eta \frac{\mathrm{d}y}{\mathrm{d}x} \mid w=w^0$$ ，再把 $$w^1$$ 更新至 $$w^2$$：$$w^2 \leftarrow w^1-\eta \frac{\mathrm{d}y}{\mathrm{d}x} \mid w=w^0$$
- 上面的操作反复经过非常多次iteration后，那么你的参数 $$w$$ 会收敛到 $$w^T$$ 这个local optimal（局部最优）的位置，因为这个位置是谷底，微分是0，卡住出不去了，所以就收敛了。如下图：<br />![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585118829339-3631e2ad-4921-4f33-af06-9779e087877c.png#align=left&display=inline&height=322&name=image.png&originHeight=1176&originWidth=2364&size=1293084&status=done&style=none&width=648)
- 但是我们发现，实际上 $$w^T$$ 这个位置并不是global optimal（全局最优），因为下图中右边的谷底才是全局最优。但实际上并不是一个问题，因为在Linear Regrassion中是没有卡在局部最优的问题的，这个事情我们等一下会再看到。![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585119094078-fd4852dd-758e-4eef-9306-067c9d9c9b10.png#align=left&display=inline&height=339&name=image.png&originHeight=1182&originWidth=2330&size=1445461&status=done&style=none&width=668)



**当有2个参数** $$w^*,b^*=arg \min_{w,b}(w,b)$$ **时GD是怎么做的：**<br />实际上跟上面并没有什么不同

- 给 $$w^0$$ 跟 $$b^0$$ 初始一个值
- 分别计算两个偏微分：<br /><br />$$\frac{\partial L}{\partial w} \mid_{w=w^0,b=b^0}$$<br /><br />$$\frac{\partial L}{\partial b} \mid_{w=w^0,b=b^0}$$
- 更新两个参数：<br /><br />$$w^1\leftarrow w^0- \eta \frac{\partial L}{\partial w} \mid_{w=w^0,b=b^0}$$<br /><br /><br />$$b^1\leftarrow b^0 -\eta \frac{\partial L}{\partial w} \mid_{w=w^0,b=b^0}$$

- 同理，反复进行这个步骤，这样就可以找到 $$w$$ 跟 $$b$$ 的local optimal



**所谓的Gradient Descent中的Gradient指的是什么呢**，指的是：<br /><br />$$\triangledown L=\begin{bmatrix} \frac{\partial L}{\partial w} \\ \frac{\partial L}{\partial b} \end{bmatrix}$$<br /><br />这个$$\triangledown L$$表示$$w$$对$$L$$的偏微分，和$$b$$对$$L$$的偏微分排成一个vector，那么这一项就是gradient。 

# 为什么不用担心Linear Regression陷入局部最优？

<br />上文中我们会有个担心是，如果Linear regression的全局最优点位在y，但假如初始化点在A的话，按照GD的走法一定会走到x这个位置从而陷入局部最优。<br />

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585123272880-2b21367b-a30d-49d8-9174-11ae32936195.png#align=left&display=inline&height=317&name=image.png&originHeight=1180&originWidth=1600&size=1661219&status=done&style=none&width=430)

<br />实际上，我们并不用担心这个问题，因为在Linear regression中，$$L$$ 是convex（没有local optimal位置）的，如果把图画出来的话，就长下面这个样子：<br />

![image.png](https://cdn.nlark.com/yuque/0/2020/png/1173836/1585123402037-de4c3dd5-8cde-487a-870f-eb47518e1398.png#align=left&display=inline&height=284&name=image.png&originHeight=684&originWidth=830&size=840212&status=done&style=none&width=345)

推导$$\partial L/\partial w$$和$$\partial L/\partial b$$：<br />先来回顾一下，loss是长这个样子的：$$L(w,b)=\sum_{n-1}^k (\widehat{y}^n-(b+w\cdot x_{cp}^n))^2$$<br />那么：<br />$$\frac{\partial L}{\partial w}=\sum_{n-1}^k 2(\widehat{y}^n-(b+w\cdot x_{cp}^n))^2(-x_{cp}^n)$$<br />$$\frac{\partial L}{\partial b}=\sum_{n-1}^k 2(\widehat{y}^n-(b+w\cdot x_{cp}^n))^2(-1)$$<br />

# Regularzation

<br />Regularzation正则化是一个可以处理overfitting的手段，它会重新定义一个function是好还是坏，也就是重新定义我们的 $$L$$，我们把一些knowledge放进去，让我们可以找到比较好的function。

假设我们的model写成 $$y=b+\sum w_{i}x_{i}$$ ，我们原来的 $$L=\sum_{n-1}^k (\widehat{y}^n-(b+w\cdot x_{cp}^n))^2$$ 只考虑了error这件事，也就是只考虑了预测结果与实际结果之间的误差，regularzation就是加上了额外的一项$$\lambda \sum (w_{i})^2$$，写作：

$$L=\sum_{n-1}^k (\widehat{y}^n-(b+w\cdot x_{cp}^n))^2 + \lambda \sum (w_{i})^2$$<br />
<br />其中 $$\lambda$$ 是我们要手调的参数（常数），$$\sum (w_{i})^2$$表示把所有的参数 $$w$$ 平方后把他们加起来。<br />当加上$$\lambda \sum (w_{i})^2$$这项后，**实际上我们的预期是希望参数值** $$w$$ **越接近0越好**


## 为什么希望参数值越接近0越好？

因为如果参数 $$w$$ 越接近0，那么这个function就越平滑。

**怎么理解这个平滑？**output对输入不是很敏感的。

**为什么我们比较喜欢一个平滑的function？**<br />假如我们现在已经有了一个比较平滑的function，那么一旦有一些噪音进入了训练样本中，就会对function带来比较小的负面影响，从而得到比较好的model。

有一个细节是，正则项$$\lambda \sum (w_{i})^2$$是不需要考虑bias $$b$$ 的，在实验中也确实是不加bias比较好，因为我们的目标是找一个比较平滑的function，而调整bias对这个function是否平滑是没有关系的。
